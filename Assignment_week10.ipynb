{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n)\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import StratifiedKFold\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "I use the UCI Spambase dataset (4601 rows, 58 columns).  \n57 numeric engineered features + 1 binary target (`class`).  \nNo text preprocessing needed.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load data from UCI CSV (raw file has no header)\nuci_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\ndf = pd.read_csv(uci_url, header=None)\ncols = [f\"f{i:02d}\" for i in range(57)]\ndf.columns = cols + [\"class\"]\ndf[\"class\"] = df[\"class\"].astype(int)\n\ndf.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Class balance: Ham=2788, Spam=1813 (~61% ham, 39% spam).  \nStandardization may help LR/SVM.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "df.info()\ndf[\"class\"].value_counts()\ndf.describe().T.head(5)\n\nplt.figure()\ndf[\"class\"].value_counts().sort_index().plot(kind=\"bar\")\nplt.title(\"Class balance\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Train/test split (80/20), stratified."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "X = df.drop(columns=[\"class\"])\ny = df[\"class\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, stratify=y\n)\n\nprint(X_train.shape, X_test.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Helper: calculates metrics for a given probability vector\ndef evaluate_predictions(y_true, y_proba, threshold=0.5):\n    y_pred = (y_proba >= threshold).astype(int)\n    return {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n        \"auc\": roc_auc_score(y_true, y_proba),\n        \"cm\": confusion_matrix(y_true, y_pred)\n    }\n\n# Store model results\nresults = {}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Logistic Regression (threshold=0.50)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lr_pipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"clf\", LogisticRegression(solver=\"liblinear\", max_iter=200))\n])\n\nlr_pipe.fit(X_train, y_train)\ny_proba_lr = lr_pipe.predict_proba(X_test)[:, 1]\nlr_metrics = evaluate_predictions(y_test, y_proba_lr, 0.50)\nresults[\"LR@0.50\"] = lr_metrics\nresults\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Confusion matrix + ROC\ncm = lr_metrics[\"cm\"]\nConfusionMatrixDisplay(cm, display_labels=[0,1]).plot()\nplt.show()\n\nfpr, tpr, _ = roc_curve(y_test, y_proba_lr)\nplt.plot(fpr, tpr, label=f\"AUC={lr_metrics['auc']:.3f}\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Threshold sweep to improve F1."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "thresholds = np.linspace(0.05, 0.95, 19)\nscores = []\nfor t in thresholds:\n    m = evaluate_predictions(y_test, y_proba_lr, t)\n    scores.append((t, m[\"precision\"], m[\"recall\"], m[\"f1\"]))\n\nthr_df = pd.DataFrame(scores, columns=[\"threshold\",\"precision\",\"recall\",\"f1\"])\nbest_threshold = thr_df.iloc[thr_df.f1.idxmax()][\"threshold\"]\n\n# Evaluate LR at best threshold\nlr_best = evaluate_predictions(y_test, y_proba_lr, best_threshold)\nresults[f\"LR@{best_threshold:.2f}\"] = lr_best\nresults\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Gaussian Naive Bayes"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "nb = GaussianNB()\nnb.fit(X_train, y_train)\ny_proba_nb = nb.predict_proba(X_test)[:, 1]\nnb_metrics = evaluate_predictions(y_test, y_proba_nb, 0.50)\nresults[\"NB@0.50\"] = nb_metrics\nresults\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Linear SVM (calibrated to get probabilities)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "svm_pipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"clf\", CalibratedClassifierCV(\n        estimator=LinearSVC(max_iter=5000),\n        cv=StratifiedKFold(n_splits=5, shuffle=True),\n        method=\"sigmoid\"\n    ))\n])\n\nsvm_pipe.fit(X_train, y_train)\ny_proba_svm = svm_pipe.predict_proba(X_test)[:, 1]\nsvm_metrics = evaluate_predictions(y_test, y_proba_svm, 0.50)\nresults[\"SVM@0.50\"] = svm_metrics\nresults\n\n# ROC for SVM\nfpr_svm, tpr_svm, _ = roc_curve(y_test, y_proba_svm)\nplt.plot(fpr_svm, tpr_svm, label=f\"AUC={svm_metrics['auc']:.3f}\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Compare models by F1 and AUC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "comparison = []\nfor name, m in results.items():\n    comparison.append([name, m[\"accuracy\"], m[\"precision\"], m[\"recall\"], m[\"f1\"], m[\"auc\"]])\n\ncomp_df = pd.DataFrame(comparison, columns=[\"model\",\"acc\",\"prec\",\"rec\",\"f1\",\"auc\"])\ncomp_df.sort_values(\"f1\", ascending=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Feature importance (from Logistic Regression)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Extract LR coefficients\nlr_clf = lr_pipe.named_steps[\"clf\"]\ncoef = lr_clf.coef_.ravel()\nfeature_names = X_train.columns\n\nfi = pd.DataFrame({\"feature\": feature_names, \"coef\": coef})\nfi[\"abs\"] = fi[\"coef\"].abs()\nfi.sort_values(\"abs\", ascending=False).head(15)\n"
    }
  ]
}